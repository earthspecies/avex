{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gnpag3641HUA"
   },
   "source": [
    "# SL-BEATs Model Demo\n",
    "\n",
    "This notebook demonstrates how to use the `representation-learning` package with the `sl-beats` model for:\n",
    "1. Classification using the original head\n",
    "2. Adding a new classification head\n",
    "3. Embedding extraction\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to authenticate and install the package from the private PyPI repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V42OzQzE1HUD"
   },
   "source": [
    "## Step 1: Authenticate with Google Cloud\n",
    "\n",
    "**For Google Colab**: You need to authenticate to access the private PyPI package.\n",
    "\n",
    "**For Local Execution**: If you're running this locally and already have `gcloud` configured, you can skip this step or run the authentication commands in your terminal first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uyk8A7aQ1HUD"
   },
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "import importlib.util\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check if we're in Colab\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "if IN_COLAB:\n",
    "    get_ipython().system(\"gcloud auth login --no-launch-browser\")\n",
    "    get_ipython().system(\"gcloud auth application-default login --no-launch-browser\")\n",
    "else:\n",
    "    print(\"Running locally. Checking gcloud authentication...\")\n",
    "    try:\n",
    "        result = subprocess.run([\"gcloud\", \"auth\", \"list\"], capture_output=True, text=True, check=False)\n",
    "        if \"ACTIVE\" in result.stdout:\n",
    "            print(\"‚úÖ Already authenticated with gcloud\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Not authenticated. Please run in terminal:\")\n",
    "            print(\"   gcloud auth login\")\n",
    "            print(\"   gcloud auth application-default login\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è  gcloud CLI not found. Please install it or authenticate manually.\")\n",
    "\n",
    "# Set up Google Cloud project for accessing checkpoints\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"okapi-274503\"  # Set the GCS project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdTWTd1F1HUE"
   },
   "source": [
    "## Step 2: Install UV and Keyring Plugin\n",
    "\n",
    "Install `uv` package manager and the Google Artifact Registry authentication plugin.\n",
    "\n",
    "**For Local Execution**: If you already have `uv` installed and the package is available locally, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t--1-m4t1HUE"
   },
   "outputs": [],
   "source": [
    "# Install uv\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "uv_installed = shutil.which(\"uv\") is not None\n",
    "\n",
    "if not uv_installed:\n",
    "    print(\"Installing uv...\")\n",
    "    try:\n",
    "        get_ipython().system(\"pip install uv\")\n",
    "    except NameError:\n",
    "        subprocess.run([\"pip\", \"install\", \"uv\"], check=False)\n",
    "else:\n",
    "    print(\"‚úÖ uv is already installed\")\n",
    "\n",
    "# Install keyring\n",
    "try:\n",
    "    get_ipython().system(\"uv tool install keyring --with keyrings.google-artifactregistry-auth\")\n",
    "except NameError:\n",
    "    subprocess.run([\"uv\", \"tool\", \"install\", \"keyring\", \"--with\", \"keyrings.google-artifactregistry-auth\"], check=False)\n",
    "except Exception:\n",
    "    print(\"‚ö†Ô∏è  Keyring installation may have failed. If running locally with the package already installed, this is OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWF2HTCC1HUE"
   },
   "source": [
    "## Step 3: Configure UV for Private PyPI\n",
    "\n",
    "Create a `pyproject.toml` configuration file to use the private index.\n",
    "\n",
    "**For Local Execution**: If you're running from the project root, this will append to existing pyproject.toml or create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9ZkZuWz1HUF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if we're in the project root (has existing pyproject.toml with project config)\n",
    "current_dir = Path.cwd()\n",
    "existing_pyproject = current_dir / \"pyproject.toml\"\n",
    "\n",
    "# Check if existing pyproject.toml has [project] section (indicates it's a real project)\n",
    "has_project_config = False\n",
    "needs_fix = False\n",
    "if existing_pyproject.exists():\n",
    "    try:\n",
    "        content = existing_pyproject.read_text()\n",
    "        if \"[project]\" in content or \"[build-system]\" in content:\n",
    "            has_project_config = True\n",
    "            print(\"‚ö†Ô∏è  Found existing pyproject.toml with project configuration\")\n",
    "            print(\"   We'll append our index configuration to it...\")\n",
    "        # Check if it has wrong format (single brackets instead of double)\n",
    "        if \"[tool.uv.index]\" in content and \"[[tool.uv.index]]\" not in content:\n",
    "            needs_fix = True\n",
    "            print(\"‚ö†Ô∏è  Found incorrect format in pyproject.toml (single brackets)\")\n",
    "            print(\"   Fixing format by replacing [tool.uv.index] with [[tool.uv.index]]...\")\n",
    "            # Fix the format\n",
    "            content = content.replace(\"[tool.uv.index]\", \"[[tool.uv.index]]\")\n",
    "            with open(\"pyproject.toml\", \"w\") as f:\n",
    "                f.write(content)\n",
    "            print(\"   ‚úÖ Fixed format in existing pyproject.toml\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not read existing pyproject.toml: {e}\")\n",
    "\n",
    "# Create/update pyproject.toml configuration\n",
    "# NOTE: Use [[tool.uv.index]] (double brackets) for array of tables - this is REQUIRED by uv\n",
    "pyproject_content = \"\"\"[[tool.uv.index]]\n",
    "name = \"esp-pypi\"\n",
    "url = \"https://oauth2accesstoken@us-central1-python.pkg.dev/okapi-274503/esp-pypi/simple/\"\n",
    "explicit = true\n",
    "\n",
    "[tool.uv.sources]\n",
    "representation-learning = { index = \"esp-pypi\" }\n",
    "\n",
    "[tool.uv]\n",
    "keyring-provider = \"subprocess\"\n",
    "\"\"\"\n",
    "\n",
    "if has_project_config and not needs_fix:\n",
    "    # Append to existing file (only if we didn't just fix it)\n",
    "    with open(\"pyproject.toml\", \"a\") as f:\n",
    "        f.write(\"\\n\")\n",
    "        f.write(pyproject_content)\n",
    "    print(\"‚úÖ Appended index configuration to existing pyproject.toml\")\n",
    "else:\n",
    "    # Create new file or overwrite if we fixed the format\n",
    "    if not has_project_config:\n",
    "        with open(\"pyproject.toml\", \"w\") as f:\n",
    "            f.write(pyproject_content)\n",
    "        print(\"‚úÖ Created pyproject.toml configuration\")\n",
    "    print(\"   Note: Using [[tool.uv.index]] (double brackets) as required by uv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HwVUTBb1HUF"
   },
   "source": [
    "## Step 4: Install representation-learning Package\n",
    "\n",
    "Install the package using `uv` with the configured private index.\n",
    "\n",
    "**Note**: We'll try `uv add` first (which reads pyproject.toml), then fall back to direct index URL if needed.\n",
    "\n",
    "**For Local Execution**: If the package is already installed in your environment (e.g., via `uv sync`), you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDILHKtL-C-R"
   },
   "outputs": [],
   "source": [
    "# Install numpy first to avoid compatibility issues\n",
    "!pip install numpy==1.26.4\n",
    "!uv pip install representation-learning --extra-index-url https://oauth2accesstoken@us-central1-python.pkg.dev/okapi-274503/esp-pypi/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO8PKJCq1HUG"
   },
   "source": [
    "## Step 5: Import and Verify Installation\n",
    "\n",
    "Import the package and verify it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmaDfmym1HUG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from representation_learning import describe_model, list_models, load_model\n",
    "from representation_learning.api import build_probe_from_config_online\n",
    "from representation_learning.configs import ProbeConfig\n",
    "\n",
    "print(\"Package imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# List available models (prints formatted table + returns dict)\n",
    "print(\"\\nAvailable models:\")\n",
    "models = list_models()\n",
    "\n",
    "# Show detailed information about the model we'll use\n",
    "print(\"\\nModel information:\")\n",
    "describe_model(\"sl_beats_animalspeak\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNpWTwWG1HUG"
   },
   "source": [
    "## Step 6: Use Case 1 - Classification with Original Head\n",
    "\n",
    "Load the `sl_beats_animalspeak` model with its original classification head from the checkpoint.\n",
    "When `num_classes=None`, the model will extract the number of classes from the checkpoint and load the trained classifier weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2flrcwu1HUG"
   },
   "outputs": [],
   "source": [
    "print(\"üöÄ Use Case 1: Classification with Original Head\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Determine device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Load model with original classifier (num_classes defaults to None, extracts from checkpoint)\n",
    "    model = load_model(\"sl_beats_animalspeak\", device=device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"\\n‚úÖ Model loaded successfully!\")\n",
    "    print(f\"   Model type: {type(model).__name__}\")\n",
    "\n",
    "    # Check if classifier exists\n",
    "    if hasattr(model, \"classifier\") and model.classifier is not None:\n",
    "        print(\"   ‚úÖ Original classifier loaded from checkpoint\")\n",
    "        print(f\"   Classifier weight shape: {model.classifier.weight.shape}\")\n",
    "        print(f\"   Classifier bias shape: {model.classifier.bias.shape}\")\n",
    "        num_classes = model.classifier.weight.shape[0]\n",
    "        print(f\"   Number of classes: {num_classes}\")\n",
    "\n",
    "        # Check for label mapping\n",
    "        if hasattr(model, \"label_mapping\") and model.label_mapping:\n",
    "            index_to_label = model.label_mapping.get(\"index_to_label\", {})\n",
    "            print(f\"   Label mapping available: {len(index_to_label)} classes\")\n",
    "            if index_to_label:\n",
    "                print(\"   Sample classes:\")\n",
    "                for idx in list(index_to_label.keys())[:5]:\n",
    "                    print(f\"     {idx}: {index_to_label[idx]}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No classifier found (model in embedding mode)\")\n",
    "\n",
    "    # Test forward pass\n",
    "    print(\"\\nüß™ Testing forward pass...\")\n",
    "    dummy_input = torch.randn(1, 16000 * 5)  # 5 seconds of audio at 16kHz\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input, padding_mask=None)\n",
    "\n",
    "    print(f\"   Input shape: {dummy_input.shape}\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "\n",
    "    if hasattr(model, \"classifier\") and model.classifier is not None:\n",
    "        # Get predictions\n",
    "        probs = torch.softmax(output, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, k=min(3, output.shape[-1]), dim=-1)\n",
    "        print(\"\\n   Top-3 predictions:\")\n",
    "        for i, (prob, idx) in enumerate(zip(top_probs[0], top_indices[0], strict=False)):\n",
    "            idx_int = idx.item()\n",
    "            if hasattr(model, \"label_mapping\") and model.label_mapping:\n",
    "                index_to_label = model.label_mapping.get(\"index_to_label\", {})\n",
    "                label = index_to_label.get(idx_int, f\"Class {idx_int}\")\n",
    "                print(f\"     {i + 1}. {label}: {prob.item():.4f}\")\n",
    "            else:\n",
    "                print(f\"     {i + 1}. Class {idx_int}: {prob.item():.4f}\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Model returns embeddings (not classification logits)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXpLVchd1HUG"
   },
   "source": [
    "## Step 7: Use Case 2 - Adding a New Classification Head with a Probe\n",
    "\n",
    "Load the model backbone and add a new linear probe head with a different number of classes.\n",
    "The backbone weights come from the checkpoint, while the probe head is initialized for your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ksAZbLO1HUG"
   },
   "outputs": [],
   "source": [
    "print(\"Use Case 2: Adding a New Classification Head with a Probe\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Load model backbone in embedding mode\n",
    "    new_num_classes = 20\n",
    "    print(f\"Creating linear probe with {new_num_classes} classes...\")\n",
    "\n",
    "    backbone = load_model(\"sl_beats_animalspeak\", return_features_only=True, device=device)\n",
    "    backbone.eval()\n",
    "\n",
    "    # Build a simple linear probe on top of the backbone\n",
    "    probe_config = ProbeConfig(\n",
    "        probe_type=\"linear\",\n",
    "        target_layers=[\"backbone\"],\n",
    "        aggregation=\"mean\",\n",
    "        freeze_backbone=True,\n",
    "        online_training=True,\n",
    "    )\n",
    "    model = build_probe_from_config_online(\n",
    "        probe_config=probe_config,\n",
    "        base_model=backbone,\n",
    "        num_classes=new_num_classes,\n",
    "        device=device,\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    print(\"\\nModel with probe head created\")\n",
    "\n",
    "    # Test forward pass\n",
    "    print(\"\\nTesting forward pass...\")\n",
    "    dummy_input = torch.randn(1, 16000 * 5)  # 5 seconds of audio\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "\n",
    "    print(f\"   Input shape: {dummy_input.shape}\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Model outputs classification logits for {new_num_classes} classes\")\n",
    "\n",
    "    print(\"\\nThis probe head can be trained for your specific task\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ze9-xTCP1HUG"
   },
   "source": [
    "## Step 8: Use Case 3 - Embedding Extraction\n",
    "\n",
    "Load the model in embedding extraction mode. This is useful for:\n",
    "- Transfer learning\n",
    "- Linear probing\n",
    "- Feature extraction for downstream tasks\n",
    "\n",
    "When loading with `return_features_only=True`, the model returns embeddings instead of classification logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-bPZjLM1HUH"
   },
   "outputs": [],
   "source": [
    "print(\"üöÄ Use Case 3: Embedding Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Load model for embedding extraction using return_features_only=True\n",
    "    print(\"Loading sl_beats_animalspeak in embedding extraction mode...\")\n",
    "    print(\"(Using return_features_only=True to extract embeddings)\")\n",
    "\n",
    "    model = load_model(\"sl_beats_animalspeak\", return_features_only=True, device=device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"\\n‚úÖ Model loaded in embedding extraction mode!\")\n",
    "\n",
    "    # Check if classifier exists\n",
    "    has_classifier = hasattr(model, \"classifier\") and model.classifier is not None\n",
    "    if has_classifier:\n",
    "        print(\"   ‚ö†Ô∏è  Model has a classifier (unexpected for embedding mode)\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Model has no classifier (embedding extraction mode)\")\n",
    "        print(f\"   Return features only: {getattr(model, '_return_features_only', 'N/A')}\")\n",
    "\n",
    "    # Test forward pass - should return embeddings\n",
    "    print(\"\\nüß™ Testing forward pass...\")\n",
    "    dummy_input = torch.randn(1, 16000 * 5)  # 5 seconds of audio\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input, padding_mask=None)\n",
    "\n",
    "    print(f\"   Input shape: {dummy_input.shape}\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   ‚úÖ Model returns embeddings (dimension: {output.shape[-1]})\")\n",
    "\n",
    "    # Show embedding statistics\n",
    "    print(\"\\nüìä Embedding statistics:\")\n",
    "    print(f\"   Mean: {output.mean().item():.4f}\")\n",
    "    print(f\"   Std: {output.std().item():.4f}\")\n",
    "    print(f\"   Min: {output.min().item():.4f}\")\n",
    "    print(f\"   Max: {output.max().item():.4f}\")\n",
    "\n",
    "    print(\"\\nüí° These embeddings can be used for:\")\n",
    "    print(\"   - Linear probing (training a simple classifier on top)\")\n",
    "    print(\"   - Similarity search\")\n",
    "    print(\"   - Clustering\")\n",
    "    print(\"   - Transfer learning to new tasks\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBWK8Hij1HUH"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated three main use cases for the `sl-beats` model:\n",
    "\n",
    "1. **Classification with Original Head**: Load the model to use the trained classifier from the checkpoint.\n",
    "\n",
    "2. **Adding a New Classification Head**: Load the model backbone and attach a new linear probe head with your desired number of classes.\n",
    "\n",
    "3. **Embedding Extraction**: Load the model with `return_features_only=True` to extract features for downstream tasks.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- `load_model(\"sl_beats_animalspeak\")`: Loads the model with its checkpoint classifier\n",
    "- `load_model(..., return_features_only=True)`: Loads the model in embedding extraction mode (no classifier)\n",
    "- Linear probe heads can be attached using `ProbeConfig` and `build_probe_from_config_online`\n",
    "- The model backbone can be accessed directly for custom feature extraction\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Train the new probe head on your dataset\n",
    "- Use embeddings for linear probing or similarity search\n",
    "- Fine-tune the entire model for your specific task"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
