import os
import argparse

from tqdm import tqdm
from representation_learning import load_model
import torch
import torchaudio  # noqa: F401  # (kept in case model uses it internally)
from torch.utils.data import Dataset, DataLoader
from esp_data.io import read_audio
import librosa
import pandas as pd
import numpy as np

MODEL_SR = 32000
TARGET_SEC = 10.

# ontology generated by chatgpt, possibly not 100% correct but ok enough
AUDIOSET_ONTOLOGY = [
    "Speech",
    "Child speech, kid speaking",
    "Conversation",
    "Narration, monologue",
    "Babbling",
    "Speech synthesizer",
    "Shout",
    "Bellow",
    "Whoop",
    "Yell",
    "Battle cry",
    "Children shouting",
    "Screaming",
    "Whispering",
    "Laughter",
    "Baby laughter",
    "Giggle",
    "Snicker",
    "Belly laugh",
    "Chuckle, chortle",
    "Crying, sobbing",
    "Baby cry, infant cry",
    "Whimper",
    "Wail, moan",
    "Sigh",
    "Singing",
    "Choir",
    "Yodeling",
    "Chant",
    "Mantra",
    "Male singing",
    "Female singing",
    "Child singing",
    "Synthetic singing",
    "Rapping",
    "Humming",
    "Groan",
    "Grunt",
    "Whistling",
    "Breathing",
    "Wheeze",
    "Snoring",
    "Gasp",
    "Pant",
    "Snort",
    "Cough",
    "Throat clearing",
    "Sneeze",
    "Sniff",
    "Run",
    "Shuffle",
    "Walk, footsteps",
    "Chewing, mastication",
    "Biting",
    "Gargling",
    "Stomach rumble",
    "Burping, eructation",
    "Hiccup",
    "Fart",
    "Hands",
    "Finger snapping",
    "Clapping",
    "Heart sounds, heartbeat",
    "Heart murmur",
    "Cheering",
    "Applause",
    "Chatter",
    "Crowd",
    "Hubbub, speech noise, speech babble",
    "Children playing",
    "Animal",
    "Domestic animals, pets",
    "Dog",
    "Bark",
    "Yip",
    "Howl",
    "Bow-wow",
    "Growling",
    "Whimper (dog)",
    "Cat",
    "Meow",
    "Purr",
    "Caterwaul",
    "Livestock, farm animals, working animals",
    "Horse",
    "Clip-clop",
    "Neigh, whinny",
    "Cattle, bovinae",
    "Moo",
    "Cowbell",
    "Pig",
    "Oink",
    "Goat",
    "Bleat",
    "Sheep",
    "Fowl",
    "Chicken, rooster",
    "Cluck",
    "Crowing, cock-a-doodle-doo",
    "Turkey",
    "Gobble",
    "Duck",
    "Quack",
    "Goose",
    "Honk",
    "Wild animals",
    "Roaring cats (lions, tigers)",
    "Roar",
    "Bird",
    "Bird vocalization, bird call, bird song",
    "Chirp, tweet",
    "Squawk",
    "Pigeon, dove",
    "Coo",
    "Crow",
    "Caw",
    "Owl",
    "Hoot",
    "Bird flight, flapping wings",
    "Canidae, dogs, wolves",
    "Rodents, rats, mice",
    "Mouse",
    "Patter",
    "Insect",
    "Cricket",
    "Mosquito",
    "Fly, housefly",
    "Bee, wasp, etc.",
    "Frog",
    "Croak",
    "Snake",
    "Rattle",
    "Whale vocalization",
    "Dolphin vocalization",
    "Sounds of things",
    "Vehicle",
    "Car",
    "Vehicle horn, car horn, honking",
    "Toot",
    "Car alarm",
    "Power windows, electric windows",
    "Skidding",
    "Tire squeal",
    "Car passing by",
    "Race car, auto racing",
    "Truck",
    "Air brake",
    "Air horn, truck horn",
    "Reversing beeps",
    "Bus",
    "Motorcycle",
    "Engine",
    "Engine knocking",
    "Engine starting",
    "Idling",
    "Accelerating, revving, vroom",
    "Door",
    "Doorbell",
    "Door knock",
    "Sliding door",
    "Slam",
    "Vehicle door",
    "Aircraft",
    "Aircraft engine",
    "Jet engine",
    "Propeller, airscrew",
    "Helicopter",
    "Fixed-wing aircraft, airplane",
    "Bicycle",
    "Skateboard",
    "Train",
    "Train whistle",
    "Train horn",
    "Railroad car, train wagon",
    "Train wheels squealing",
    "Subway, metro, underground",
    "Boat, water vehicle",
    "Motorboat, speedboat",
    "Ship",
    "Sailboat, sailing ship",
    "Rowboat, canoe, kayak",
    "Alarm",
    "Buzzer",
    "Smoke detector, smoke alarm",
    "Fire alarm",
    "Siren",
    "Civil defense siren",
    "Police car (siren)",
    "Ambulance (siren)",
    "Fire engine, fire truck (siren)",
    "Traffic noise, roadway noise",
    "Construction noise",
    "Jackhammer",
    "Drill",
    "Power tool",
    "Sawing",
    "Hammer",
    "Wood",
    "Chop",
    "Splinter",
    "Crack",
    "Glass",
    "Chink, clink",
    "Shatter",
    "Metal",
    "Clang",
    "Squeak",
    "Rattle",
    "Tools",
    "Whir",
    "Mechanical fan",
    "Air conditioning",
    "Vacuum cleaner",
    "Printer",
    "Cash register",
    "Typing",
    "Typewriter",
    "Computer keyboard",
    "Mouse click",
    "Music",
    "Musical instrument",
    "Piano",
    "Electric piano",
    "Organ",
    "Synthesizer",
    "Sampler",
    "Harpsichord",
    "Percussion",
    "Drum kit",
    "Snare drum",
    "Bass drum",
    "Timpani",
    "Cymbal",
    "Hi-hat",
    "Wood block",
    "Tambourine",
    "Maraca",
    "Castanets",
    "Claves",
    "Vibraphone",
    "Marimba, xylophone",
    "Glockenspiel",
    "Steelpan",
    "Melodic percussion",
    "String instrument",
    "Guitar",
    "Electric guitar",
    "Bass guitar",
    "Acoustic guitar",
    "Steel guitar, slide guitar",
    "Tapping (guitar technique)",
    "Violin, fiddle",
    "Viola",
    "Cello",
    "Double bass",
    "Banjo",
    "Mandolin",
    "Ukulele",
    "Harp",
    "Sitar",
    "Lute",
    "Zither",
    "Bow-string instrument",
    "Wind instrument",
    "Flute",
    "Recorder",
    "Piccolo",
    "Pan flute",
    "Clarinet",
    "Oboe",
    "Bassoon",
    "Saxophone",
    "Trumpet",
    "Trombone",
    "French horn",
    "Tuba",
    "Brass instrument",
    "Bagpipes",
    "Harmonica",
    "Accordion",
    "Didgeridoo",
    "Shofar",
    "Theremin",
    "Singing bowl",
    "Bell",
    "Church bell",
    "Tubular bells",
    "Jingle bell",
    "Tuning fork",
    "Music genre",
    "Classical music",
    "Opera",
    "Symphony",
    "Chamber music",
    "Jazz",
    "Blues",
    "Rock music",
    "Heavy metal",
    "Punk rock",
    "Progressive rock",
    "Pop music",
    "Country music",
    "Folk music",
    "Hip hop music",
    "Electronic music",
    "House music",
    "Techno",
    "Trance music",
    "Dubstep",
    "Drum and bass",
    "Ambient music",
    "Reggae",
    "Ska",
    "Soul music",
    "Funk",
    "Disco",
    "R&B",
    "Gospel music",
    "Latin music",
    "Salsa music",
    "Flamenco",
    "Tango",
    "African music",
    "Middle Eastern music",
    "Indian music",
    "Carnatic music",
    "Hindustani classical music",
    "Music of Bollywood",
    "Music of Africa",
    "Music of Asia",
    "Music of Europe",
    "Music of Latin America",
    "Music of North America",
    "Music of Oceania",
    "Sound effect",
    "Whoosh, swoosh, swish",
    "Explosion",
    "Gunshot, gunfire",
    "Machine gun",
    "Artillery fire",
    "Fireworks",
    "Boom",
    "Bang",
    "Thump",
    "Footstep",
    "Heartbeat",
    "Breathing sound",
    "Wind",
    "Rain",
    "Thunder",
    "Thunderstorm",
    "Lightning",
    "Water",
    "Ocean",
    "Waves, surf",
    "Stream, river",
    "Waterfall",
    "Drip",
    "Splash, splatter",
    "Bubble",
    "Pour",
    "Steam",
    "Fire",
    "Crackle",
    "Forest",
    "Jungle",
    "Birdsong",
    "Urban environment",
    "City noise",
    "Park",
    "Crowded space",
    "Room tone",
    "Silence",
]
AUDIOSET_ONTOLOGY = [x.lower() for x in AUDIOSET_ONTOLOGY]

# -----------------------
# CLI args for sharding
# -----------------------
parser = argparse.ArgumentParser()
parser.add_argument(
    "--shard",
    type=int,
    default=0,
    help="Which shard index to process.",
)
parser.add_argument(
    "--num_shards",
    type=int,
    default=12,
    help="Total number of shards.",
)
args = parser.parse_args()
SHARD = args.shard
NUM_SHARDS = args.num_shards

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

label_to_idx = pd.read_json(
    "gs://representation-learning/models/v1/effnet_32khz_v0/label_map.json",
    typ="series",
).to_dict()
idx_to_label = {label_to_idx[x]: x for x in label_to_idx}

# get idxs not corresponding to audioset classes
bio_idxs = []
for x in idx_to_label:
    label = idx_to_label[x]
    if label.lower() not in AUDIOSET_ONTOLOGY:
        bio_idxs.append(x)

model = load_model('effnet_32khz_v0', checkpoint_path="gs://representation-learning/models/v1/effnet_32khz_v0/final_model.pt", device=device)

# model = load_model(
#     "sl_beats_all",
#     checkpoint_path="gs://representation-learning/models/v1/beats_32khz.pt",
#     num_classes=None,
# )

# -----------------------
# Load + filter metadata
# -----------------------
metadata = pd.read_csv('gs://fewshot/data_large_clean/silentcities_sampled_big_info.csv')

# -----------------------
# Compute shard slice
# -----------------------
n = len(metadata)
if NUM_SHARDS <= 0:
    raise ValueError(f"num_shards must be > 0, got {NUM_SHARDS}")

shard_size = (n + NUM_SHARDS - 1) // NUM_SHARDS  # ceil division
start = SHARD * shard_size
end = min(start + shard_size, n)

if start >= n:
    raise ValueError(
        f"Shard {SHARD} is empty: start={start}, n={n}, num_shards={NUM_SHARDS}"
    )

shard_metadata = metadata.iloc[start:end].copy()
print(
    f"Processing shard {SHARD}/{NUM_SHARDS} "
    f"with rows [{start}, {end}) out of {n} total "
    f"({len(shard_metadata)} rows in this shard)."
)


def pad_and_crop(audio: torch.Tensor, target_dur_sec: float, sr: int) -> torch.Tensor:
    """
    Center pad or center crop a 1-D audio tensor to `target_dur_sec`.

    Parameters
    ----------
    audio : torch.Tensor
        1-D tensor of shape [T]. If shape is [1, T], pass audio.squeeze(0).
    target_dur_sec : float
        Target duration in seconds, e.g. 3.0
    sr : int
        Sample rate, e.g. 16000

    Raises
    -------
    ValueError
        If not right shape

    Returns
    -------
    torch.Tensor
        1-D tensor of shape [target_len].
    """
    if audio.dim() != 1:
        raise ValueError(
            "Expected a 1-D tensor for `audio`. Got shape: {}".format(audio.shape)
        )

    target_len = int(round(target_dur_sec * sr))
    T = audio.shape[0]

    # Case 1: Exact length
    if T == target_len:
        return audio

    # Case 2: Pad (audio shorter)
    if T < target_len:
        pad_total = target_len - T
        pad_left = pad_total // 2
        pad_right = pad_total - pad_left

        out = torch.zeros(target_len, dtype=audio.dtype, device=audio.device)
        out[pad_left : pad_left + T] = audio
        return out

    # Case 3: Crop (audio longer)
    extra = T - target_len
    crop_left = extra // 2
    crop_right = crop_left + target_len
    return audio[crop_left:crop_right]


class ClipDataset(Dataset):
    def __init__(self, metadata: pd.DataFrame):
        """
        Parameters
        ----------
        metadata : pandas.DataFrame
            Must contain a column 'audio_fp'.
        """
        self.metadata = metadata
        # Work with positional indices internally, but remember the DataFrame index
        self._df_index = self.metadata.index.to_list()

    def __len__(self):
        return len(self._df_index)

    def __getitem__(self, i):
        # Map from positional index -> actual DataFrame index
        df_idx = self._df_index[i]
        row = self.metadata.loc[df_idx]

        audio_fp = row["audio_fp"]
        try:
            waveform, sr = read_audio(audio_fp)  # hack: missing / corrupt files get skipped
            qf_exclude = False
        except:
            qf_exclude = True
            waveform = np.zeros((int(MODEL_SR * TARGET_SEC),), )
            sr = MODEL_SR


        if sr != MODEL_SR:
            waveform = librosa.resample(waveform, orig_sr=sr, target_sr=MODEL_SR)
            sr = MODEL_SR

        waveform = torch.tensor(waveform)

        waveform = pad_and_crop(waveform, TARGET_SEC, MODEL_SR)

        return df_idx, waveform, qf_exclude  # return df index so we can write back later

# -------------------------------
# Inference + writeback into shard
# -------------------------------

model = model.to(device)
model.eval()

dataset = ClipDataset(shard_metadata)
loader = DataLoader(
    dataset,
    batch_size=64,   # adjust as needed
    shuffle=False,
    num_workers=12,  # adjust for your system
)

# Make sure the columns exist for this shard
shard_metadata["max_bio_logit"] = None

with torch.no_grad():
    for df_indices, batch_waveforms, qf_excludes in tqdm(loader):
        batch_waveforms = batch_waveforms.to(device, dtype=torch.float32)

        logits = model(batch_waveforms, None)  # expected shape [B, num_classes]
        logits = logits[:, bio_idxs] # mask to only bio idxs

        logits_cpu = logits.detach().cpu()
        logits_np = logits_cpu.numpy()

        # Write back to shard_metadata DataFrame
        for df_idx, logit_vec, qf_exclude in zip(df_indices, logits_np, qf_excludes):
            df_idx = int(df_idx)
            maxscore = np.amax(logit_vec)
            if qf_exclude:
                maxscore = -16.0
            shard_metadata.at[df_idx, "max_bio_logit"] = maxscore

# -----------------------
# Save shard-only output
# -----------------------
out_csv = (
    "silentcities_sampled_big_with_classifications"
    f"_shard{SHARD}.csv"
)
shard_metadata.to_csv(out_csv)
os.system(
    "gsutil -m cp -r "
    f"{out_csv} "
    "gs://fewshot/data_large_clean"
)
print(f"Wrote shard {SHARD} to {out_csv} and copied to GCS.")
