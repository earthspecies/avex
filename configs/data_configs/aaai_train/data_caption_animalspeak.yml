train_datasets:
  # AnimalSpeak dataset for training (keep original canonical_name field)
  - dataset_name: animalspeak
    dataset_version: 0.0
    split: train
    balance: false
    balance_attribute: canonical_name
    custom_balancing: false
    balancing_method: upsample
    subset_percentage: 1.0
    audio_max_length_seconds: 10
    audio_path_col: path
    data_root: ../../milad_earthspecies_org/data-migration/marius-highmem/mnt/foundation-model-data/audio_16k/
    sample_rate: 16000

val_datasets:
  # Only use AnimalSpeak for validation (keep original canonical_name field)
  - dataset_name: animalspeak
    dataset_version: 0.0
    split: validation
    balance: false
    balance_attribute: canonical_name
    custom_balancing: false
    balancing_method: upsample
    subset_percentage: 1.0
    audio_max_length_seconds: 10
    audio_path_col: path
    data_root: ../../milad_earthspecies_org/data-migration/marius-highmem/mnt/foundation-model-data/audio_16k/
    sample_rate: 16000

# Concatenation settings
concatenate_train: true
concatenate_val: true
concatenate_method: soft

# Global transformations applied AFTER concatenation
transformations:
  # Generate human-readable captions for CLIP/CLAP training
  - type: text_label_from_features
    features: [caption, caption2, species_common, canonical_name, taxonomic_name]
    output_feature: text_label
    listify: true
    override: true

  - type: deduplicate
    subset: ['local_path']
