# On which datasets to run the evaluation and which metrics to use
dataset_config: configs/data_configs/benchmark_test.yml

training_params:
  train_epochs: 1
  lr: 0.001
  batch_size: 1
  optimizer: adam
  weight_decay: 0.0001
  amp: false
  amp_dtype: bf16

# Which experiments to evaluate - testing our new flexible probing system
experiments:
  - run_name: linear_probe_mean
    run_config: configs/run_configs/pretrained/eat_base.yml
    layers: last_layer
    pretrained: true

  - run_name: linear_probe_max
    run_config: configs/run_configs/pretrained/eat_base.yml
    probe_config:
      name: linear_max
      probe_type: linear
      aggregation: max
      input_processing: pooled
      target_layers: [last_layer]
      freeze_backbone: true
      learning_rate: 0.001
      batch_size: 1
      train_epochs: 1
      optimizer: adam
      weight_decay: 0.0001
    pretrained: true

  - run_name: mlp_probe_mean
    run_config: configs/run_configs/pretrained/eat_base.yml
    probe_config:
      name: mlp_mean
      probe_type: mlp
      aggregation: mean
      input_processing: pooled
      target_layers: [last_layer]
      freeze_backbone: true
      hidden_dims: [256, 128]
      dropout_rate: 0.1
      activation: relu
      learning_rate: 0.001
      batch_size: 1
      train_epochs: 1
      optimizer: adam
      weight_decay: 0.0001
    pretrained: true

  - run_name: finetune_linear
    run_config: configs/run_configs/pretrained/eat_base.yml
    probe_config:
      name: finetune_linear
      probe_type: linear
      aggregation: mean
      input_processing: pooled
      target_layers: [last_layer]
      freeze_backbone: false
      learning_rate: 0.0001
      batch_size: 1
      train_epochs: 1
      optimizer: adam
      weight_decay: 0.0001
    pretrained: true

  # BEATs model experiments
  - run_name: beats_linear_probe_mean
    run_config: configs/run_configs/pretrained/beats_pretrained.yml
    probe_config:
      name: beats_linear_mean
      probe_type: linear
      aggregation: mean
      input_processing: pooled
      target_layers: [backbone]
      freeze_backbone: true
      learning_rate: 0.001
      batch_size: 1
      train_epochs: 1
      optimizer: adam
      weight_decay: 0.0001
    pretrained: true

  - run_name: beats_mlp_probe_max
    run_config: configs/run_configs/pretrained/beats_pretrained.yml
    probe_config:
      name: beats_mlp_max
      probe_type: mlp
      aggregation: max
      input_processing: pooled
      target_layers: [backbone]
      freeze_backbone: true
      hidden_dims: [512, 256]
      dropout_rate: 0.1
      activation: relu
      learning_rate: 0.001
      batch_size: 1
      train_epochs: 1
      optimizer: adam
      weight_decay: 0.0001
    pretrained: true

  # EfficientNet model experiments
  - run_name: efficientnet_linear_probe_mean
    run_config: configs/run_configs/pretrained/efficientnet_base.yml
    probe_config:
      name: efficientnet_linear_mean
      probe_type: linear
      aggregation: mean
      input_processing: pooled
      target_layers: [model.avgpool]
      freeze_backbone: true
      learning_rate: 0.001
      batch_size: 1
      train_epochs: 1
      optimizer: adam
      weight_decay: 0.0001
    pretrained: true

  - run_name: efficientnet_lstm_probe_none
    run_config: configs/run_configs/pretrained/efficientnet_base.yml
    probe_config:
      name: efficientnet_lstm_none
      probe_type: lstm
      aggregation: none
      input_processing: sequence
      target_layers: [model.avgpool]
      freeze_backbone: true
      lstm_hidden_size: 256
      num_layers: 2
      bidirectional: true
      dropout_rate: 0.1
      learning_rate: 0.001
      batch_size: 1
      train_epochs: 1
      optimizer: adam
      weight_decay: 0.0001
    pretrained: true

# Where to save the evaluation results
save_dir: /tmp/flexible_probing_test

device: cpu
seed: 42
num_workers: 0

# Evaluation phases
eval_modes:
  - probe
  - retrieval

overwrite_embeddings: true

# HDF5 storage configuration
hdf5_compression: gzip
hdf5_compression_level: 1
