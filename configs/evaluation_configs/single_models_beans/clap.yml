# On which datasets to run the evaluation and which metrics to use
dataset_config: configs/data_configs/benchmark.yml

training_params:
  train_epochs: 900
  lr: 0.001
  batch_size: 32
  optimizer: adamw
  weight_decay: 0.01
  amp: false
  amp_dtype: bf16

# Which experiments to evaluate
experiments:
  - run_name: clap_all_captions
    # run_config: configs/run_configs/clip_base_beans.yml
    run_config: configs/run_configs/aaai_train/clap_efficientnet_captions_h100.yml
    ### which layers to probe for embeddings
    layers: audio_projection
    ### whether to use pretrained model or the one we train on bioacoustic data
    # checkpoint_path: runs/aaai/clap_efficientnet_captions_animalspeak/2025-07-15_15-17-22/checkpoint_epoch_049.pt
    checkpoint_path: runs/aaai/clap_efficientnet_captions/2025-07-12_05-21-17/checkpoint_epoch_049.pt


# Where to save the evaluation results
save_dir: evaluation_results/clap

# Optional: Append results to a global CSV file for cross-model comparison
results_csv_path: evaluation_results/all_models_results.csv

device: cuda

seed: 42

num_workers: 8  # Enable workers for better memory management

# Evaluation phases
eval_modes:
  # - linear_probe
  - retrieval
  - clustering

overwrite_embeddings: true
