---
# OpenBEATs fine-tuning on AnimalSpeak2 dataset
#
# Training schedule (from OpenBEATs paper):
#   - Stage 1 (epochs 1-2): Backbone frozen, only classifier trains
#   - Stage 2 (epochs 3-10): Full model fine-tuning with lower LR
#
# Available pretrained checkpoints:
#   - openbeats-base-i3 (~90M parameters)
#   - openbeats-large-i3 (~317M parameters)
model_spec:
  name: openbeats
  pretrained: true
  model_id: "openbeats-large-i3"  # HuggingFace checkpoint to load
  model_size: "large"             # base (~90M), large (~317M)
  device: cuda
  audio_config:
    sample_rate: 16000
    representation: raw             # OpenBEATs expects raw waveform
    normalize: false
    target_length_seconds: 10
    window_selection: random

# Dataset configuration
dataset_config: configs/data_configs/data_base.yml

# Output directory for checkpoints and logs
output_dir: "./runs/openbeats_ft"
run_name: openbeats_ft_large
label_type: supervised

# Audio preprocessing (none â€“ handled inside the model)
preprocessing: null

# Target sample rate expected by the model
sr: 16000
debug_mode: false

# Log training runs to MLflow
logging: wandb

# Path to checkpoint to resume from (set to null or a path)
resume_from_checkpoint: null

training_params:
  train_epochs: 10
  lr: 0.0001
  batch_size: 8
  optimizer: adamw
  weight_decay: 0.01
  amp: true
  amp_dtype: bf16
  # Two-stage training: freeze backbone for first 2 epochs, then finetune for 8 more
  freeze_backbone_epochs: 2
  second_stage_lr: 0.0001  

# Learning rate scheduler configuration
scheduler:
  name: cosine
  warmup_steps: 500
  min_lr: 1e-6

# Loss for classification
loss_function: cross_entropy

# General settings
seed: 42
num_workers: 4

# W&B logging (optional)
wandb_project: representation_learning

augmentations:
  # Add background noise at random SNRs
  - noise:
      noise_dirs:
        - "/home/milad_earthspecies_org/data-migration/lambda/home/ubuntu/\
          foundation-model-storage/foundation-model-data/audio_16k/noise/demand_10s"
        - "/home/milad_earthspecies_org/data-migration/lambda/home/ubuntu/\
          foundation-model-storage/foundation-model-data/audio_16k/noise/idmt"
        - "/home/milad_earthspecies_org/data-migration/lambda/home/ubuntu/\
          foundation-model-storage/foundation-model-data/audio_16k/noise/tut2016_10s"
        - "/home/milad_earthspecies_org/data-migration/lambda/home/ubuntu/\
          foundation-model-storage/foundation-model-data/audio_16k/noise/urbansound"
        - "/home/milad_earthspecies_org/data-migration/lambda/home/ubuntu/\
          foundation-model-storage/foundation-model-data/audio_16k/noise/freesound_10s"
        - "/home/milad_earthspecies_org/data-migration/lambda/home/ubuntu/\
          foundation-model-storage/foundation-model-data/audio_16k/noise/\
          orcasound_shipnoise_10s"
        - "/home/milad_earthspecies_org/data-migration/lambda/home/ubuntu/\
          foundation-model-storage/foundation-model-data/audio_16k/noise/deepship_10s"
        - "/home/milad_earthspecies_org/data-migration/lambda/home/ubuntu/\
          foundation-model-storage/foundation-model-data/audio_16k/noise/shipsear_10s"
        - "/home/milad_earthspecies_org/data-migration/lambda/home/ubuntu/\
          foundation-model-storage/foundation-model-data/audio_16k/noise/wham_noise"
        - "/home/milad_earthspecies_org/data-migration/lambda/home/ubuntu/\
          foundation-model-storage/foundation-model-data/audio_16k/noise/audioset"
      snr_db_range: [-5, 20]  # mix noise between -5 dB and 20 dB SNR
      augmentation_prob: 0.5
  # Mixup augmentation for audio samples
  - mixup:
      alpha: 0.4  # Controls the Beta distribution shape
      n_mixup: 1  # Maximum number of mixup pairs per batch (defaults to 1)
      augmentation_prob: 0.5  # Probability of applying mixup to a batch
