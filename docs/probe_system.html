<!doctype html>
<html class="no-js" lang="en" data-content_root="./">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="genindex.html"><link rel="search" title="Search" href="search.html"><link rel="next" title="Probe API Documentation" href="api_probes.html"><link rel="prev" title="Examples" href="examples.html">
        <link rel="prefetch" href="_static/logo_light_mode.png" as="image">
        <link rel="prefetch" href="_static/logo_dark_mode.png" as="image">

    <link rel="shortcut icon" href="_static/favicon.svg"><!-- Generated with Sphinx 9.1.0 and Furo 2025.12.19 -->
        <title>Flexible Probe System - Earth Species Project - AVEX Documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?v=7bdb33bb" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=3b183883" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  --color-brand-primary: #007388;
  --color-brand-content: #007388;
  --color-brand-visited: #7BC4AD;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #5ecad6;
  --color-brand-content: #5ecad6;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #5ecad6;
  --color-brand-content: #5ecad6;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">Earth Species Project - AVEX Documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="_static/logo_light_mode.png" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="_static/logo_dark_mode.png" alt="Dark Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Core Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="supported_models.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="training_evaluation.html">Training and Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="embedding_extraction.html">Embedding Extraction and Feature Representations</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Topics</span></p>
<ul class="current">
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Flexible Probe System</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_probes.html">Probe API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_model_registration.html">Custom Model Registration Guide</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="https://github.com/earthspecies/avex" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="flexible-probe-system">
<h1>Flexible Probe System<a class="headerlink" href="#flexible-probe-system" title="Link to this heading">¶</a></h1>
<p>This document describes the new flexible probe system that replaces the old rigid linear probe approach. The new system allows you to configure different types of probes with various aggregation and processing strategies.</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">¶</a></h2>
<p>The new probe system provides:</p>
<ul class="simple">
<li><p><strong>Multiple probe types</strong>: Linear, MLP, LSTM, Attention, and Transformer probes, plus weighted versions of each</p></li>
<li><p><strong>Flexible aggregation</strong>: Mean, max, concatenation, CLS token, or no aggregation</p></li>
<li><p><strong>Input processing options</strong>: Flatten, sequence, pooled, or no processing</p></li>
<li><p><strong>Probe-specific parameters</strong>: Hidden dimensions, attention heads, LSTM configuration, etc.</p></li>
<li><p><strong>Training overrides</strong>: Per-probe learning rates, batch sizes, epochs, etc.</p></li>
<li><p><strong>Backward compatibility</strong>: Legacy configurations still work automatically</p></li>
</ul>
</section>
<section id="probe-types">
<h2>Probe Types<a class="headerlink" href="#probe-types" title="Link to this heading">¶</a></h2>
<section id="linear-probe-linear">
<h3>1. Linear Probe (<code class="docutils literal notranslate"><span class="pre">&quot;linear&quot;</span></code>)<a class="headerlink" href="#linear-probe-linear" title="Link to this heading">¶</a></h3>
<p>Simple linear classification layer. Good for baseline performance.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;linear&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;mean&quot;</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pooled&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
</section>
<section id="mlp-probe-mlp">
<h3>2. MLP Probe (<code class="docutils literal notranslate"><span class="pre">&quot;mlp&quot;</span></code>)<a class="headerlink" href="#mlp-probe-mlp" title="Link to this heading">¶</a></h3>
<p>Multi-layer perceptron with configurable hidden dimensions.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;mlp&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;mean&quot;</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pooled&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">hidden_dims</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">512</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">256</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">dropout_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.2</span>
<span class="w">  </span><span class="nt">activation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;gelu&quot;</span>
</pre></div>
</div>
</section>
<section id="lstm-probe-lstm">
<h3>3. LSTM Probe (<code class="docutils literal notranslate"><span class="pre">&quot;lstm&quot;</span></code>)<a class="headerlink" href="#lstm-probe-lstm" title="Link to this heading">¶</a></h3>
<p>Long Short-Term Memory network for sequence modeling.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;lstm&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;none&quot;</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_6&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_10&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">lstm_hidden_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="w">  </span><span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">bidirectional</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">max_sequence_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
</pre></div>
</div>
</section>
<section id="attention-probe-attention">
<h3>4. Attention Probe (<code class="docutils literal notranslate"><span class="pre">&quot;attention&quot;</span></code>)<a class="headerlink" href="#attention-probe-attention" title="Link to this heading">¶</a></h3>
<p>Attention mechanism for sequence modeling.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;attention&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;none&quot;</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_6&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_10&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">num_heads</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">  </span><span class="nt">attention_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512</span>
<span class="w">  </span><span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">max_sequence_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">800</span>
<span class="w">  </span><span class="nt">use_positional_encoding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</section>
<section id="transformer-probe-transformer">
<h3>5. Transformer Probe (<code class="docutils literal notranslate"><span class="pre">&quot;transformer&quot;</span></code>)<a class="headerlink" href="#transformer-probe-transformer" title="Link to this heading">¶</a></h3>
<p>Full transformer architecture for complex sequence modeling.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;transformer&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;none&quot;</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_4&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_6&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_10&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">num_heads</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">12</span>
<span class="w">  </span><span class="nt">attention_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">768</span>
<span class="w">  </span><span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">  </span><span class="nt">max_sequence_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1200</span>
<span class="w">  </span><span class="nt">use_positional_encoding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</section>
</section>
<section id="weighted-probe-types">
<h2>Weighted Probe Types<a class="headerlink" href="#weighted-probe-types" title="Link to this heading">¶</a></h2>
<p>Weighted probe types are enhanced versions of the standard probes that use learned weights to combine multiple layer embeddings. They provide a single architecture head that learns optimal weights for combining embeddings from different layers.</p>
<section id="weighted-linear-probe-weighted-linear">
<h3>6. Weighted Linear Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_linear&quot;</span></code>)<a class="headerlink" href="#weighted-linear-probe-weighted-linear" title="Link to this heading">¶</a></h3>
<p>Single linear classifier with learned weights for combining multiple layer embeddings.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;weighted_linear&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;none&quot;</span><span class="w">  </span><span class="c1"># Required for weighted probes</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pooled&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_6&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_10&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">freeze_backbone</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</section>
<section id="weighted-mlp-probe-weighted-mlp">
<h3>7. Weighted MLP Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_mlp&quot;</span></code>)<a class="headerlink" href="#weighted-mlp-probe-weighted-mlp" title="Link to this heading">¶</a></h3>
<p>Single MLP with learned weights for combining multiple layer embeddings.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;weighted_mlp&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;none&quot;</span><span class="w">  </span><span class="c1"># Required for weighted probes</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pooled&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_6&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_10&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">hidden_dims</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">512</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">256</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">dropout_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.2</span>
<span class="w">  </span><span class="nt">activation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;gelu&quot;</span>
<span class="w">  </span><span class="nt">freeze_backbone</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</section>
<section id="weighted-lstm-probe-weighted-lstm">
<h3>8. Weighted LSTM Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_lstm&quot;</span></code>)<a class="headerlink" href="#weighted-lstm-probe-weighted-lstm" title="Link to this heading">¶</a></h3>
<p>Single LSTM with learned weights for combining multiple layer embeddings.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;weighted_lstm&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;none&quot;</span><span class="w">  </span><span class="c1"># Required for weighted probes</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_4&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_6&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_10&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">lstm_hidden_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
<span class="w">  </span><span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">bidirectional</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">max_sequence_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
<span class="w">  </span><span class="nt">use_positional_encoding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">  </span><span class="nt">dropout_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.3</span>
<span class="w">  </span><span class="nt">freeze_backbone</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</section>
<section id="weighted-attention-probe-weighted-attention">
<h3>9. Weighted Attention Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_attention&quot;</span></code>)<a class="headerlink" href="#weighted-attention-probe-weighted-attention" title="Link to this heading">¶</a></h3>
<p>Single attention mechanism with learned weights for combining multiple layer embeddings.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;weighted_attention&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;none&quot;</span><span class="w">  </span><span class="c1"># Required for weighted probes</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_4&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_6&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_10&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">num_heads</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">  </span><span class="nt">attention_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="w">  </span><span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">max_sequence_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">800</span>
<span class="w">  </span><span class="nt">use_positional_encoding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">  </span><span class="nt">dropout_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.3</span>
<span class="w">  </span><span class="nt">freeze_backbone</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</section>
<section id="weighted-minimal-attention-probe-weighted-attention-minimal">
<h3>10. Weighted Minimal Attention Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_attention_minimal&quot;</span></code>)<a class="headerlink" href="#weighted-minimal-attention-probe-weighted-attention-minimal" title="Link to this heading">¶</a></h3>
<p>Single minimal attention mechanism with learned weights for combining multiple layer embeddings.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;weighted_attention_minimal&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;none&quot;</span><span class="w">  </span><span class="c1"># Required for weighted probes</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_6&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_10&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">num_heads</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">  </span><span class="nt">freeze_backbone</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</section>
<section id="weighted-transformer-probe-weighted-transformer">
<h3>11. Weighted Transformer Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_transformer&quot;</span></code>)<a class="headerlink" href="#weighted-transformer-probe-weighted-transformer" title="Link to this heading">¶</a></h3>
<p>Single transformer encoder with learned weights for combining multiple layer embeddings.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;weighted_transformer&quot;</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;none&quot;</span><span class="w">  </span><span class="c1"># Required for weighted probes</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence&quot;</span>
<span class="w">  </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_4&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_6&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_10&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">num_heads</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">12</span>
<span class="w">  </span><span class="nt">attention_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">768</span>
<span class="w">  </span><span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">  </span><span class="nt">max_sequence_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1200</span>
<span class="w">  </span><span class="nt">use_positional_encoding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">dropout_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.3</span>
<span class="w">  </span><span class="nt">freeze_backbone</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</section>
<section id="key-features-of-weighted-probes">
<h3>Key Features of Weighted Probes<a class="headerlink" href="#key-features-of-weighted-probes" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Single Architecture Head</strong>: Each weighted probe uses one architecture component (linear, MLP, LSTM, attention, transformer) instead of multiple projection heads per layer</p></li>
<li><p><strong>Learned Weighted Sum</strong>: Uses <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code> to learn optimal weights for combining multiple layer embeddings</p></li>
<li><p><strong>Dimension Validation</strong>: Ensures all embeddings have the same dimension for weighted sum aggregation</p></li>
<li><p><strong>Weight Debugging</strong>: All weighted probes implement <code class="docutils literal notranslate"><span class="pre">print_learned_weights()</span></code> method to show which layers are most important</p></li>
<li><p><strong>Efficiency</strong>: More efficient than multiple projection heads while maintaining or improving performance</p></li>
</ul>
</section>
<section id="requirements-for-weighted-probes">
<h3>Requirements for Weighted Probes<a class="headerlink" href="#requirements-for-weighted-probes" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Aggregation</strong>: Must use <code class="docutils literal notranslate"><span class="pre">aggregation:</span> <span class="pre">&quot;none&quot;</span></code> to enable learned weights</p></li>
<li><p><strong>Multiple Layers</strong>: Requires multiple target layers to learn meaningful weights</p></li>
<li><p><strong>Same Dimensions</strong>: All layer embeddings must have the same dimension for weighted sum</p></li>
</ul>
</section>
</section>
<section id="aggregation-methods">
<h2>Aggregation Methods<a class="headerlink" href="#aggregation-methods" title="Link to this heading">¶</a></h2>
<section id="mean">
<h3><code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code><a class="headerlink" href="#mean" title="Link to this heading">¶</a></h3>
<p>Average embeddings across layers (default for backward compatibility).</p>
</section>
<section id="max">
<h3><code class="docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code><a class="headerlink" href="#max" title="Link to this heading">¶</a></h3>
<p>Take maximum values across layers.</p>
</section>
<section id="concat">
<h3><code class="docutils literal notranslate"><span class="pre">&quot;concat&quot;</span></code><a class="headerlink" href="#concat" title="Link to this heading">¶</a></h3>
<p>Concatenate embeddings from all layers (requires larger probe networks).</p>
</section>
<section id="cls-token">
<h3><code class="docutils literal notranslate"><span class="pre">&quot;cls_token&quot;</span></code><a class="headerlink" href="#cls-token" title="Link to this heading">¶</a></h3>
<p>Use only the CLS token from sequence-based models.</p>
</section>
<section id="none">
<h3><code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code><a class="headerlink" href="#none" title="Link to this heading">¶</a></h3>
<p>No aggregation - pass embeddings directly to sequence-based probes.</p>
</section>
</section>
<section id="input-processing-methods">
<h2>Input Processing Methods<a class="headerlink" href="#input-processing-methods" title="Link to this heading">¶</a></h2>
<section id="pooled">
<h3><code class="docutils literal notranslate"><span class="pre">&quot;pooled&quot;</span></code><a class="headerlink" href="#pooled" title="Link to this heading">¶</a></h3>
<p>Pool embeddings to fixed dimension (default for backward compatibility).</p>
</section>
<section id="sequence">
<h3><code class="docutils literal notranslate"><span class="pre">&quot;sequence&quot;</span></code><a class="headerlink" href="#sequence" title="Link to this heading">¶</a></h3>
<p>Keep sequence structure for sequence-based probes.</p>
</section>
<section id="flatten">
<h3><code class="docutils literal notranslate"><span class="pre">&quot;flatten&quot;</span></code><a class="headerlink" href="#flatten" title="Link to this heading">¶</a></h3>
<p>Flatten all dimensions into a single vector.</p>
</section>
<section id="id1">
<h3><code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code><a class="headerlink" href="#id1" title="Link to this heading">¶</a></h3>
<p>No processing - use embeddings as-is.</p>
</section>
</section>
<section id="configuration-examples">
<h2>Configuration Examples<a class="headerlink" href="#configuration-examples" title="Link to this heading">¶</a></h2>
<section id="basic-linear-probe-legacy-style">
<h3>Basic Linear Probe (Legacy Style)<a class="headerlink" href="#basic-linear-probe-legacy-style" title="Link to this heading">¶</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">experiments</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">run_name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;simple_linear&quot;</span>
<span class="w">    </span><span class="nt">run_config</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;configs/run_configs/example_run.yml&quot;</span>
<span class="w">    </span><span class="nt">pretrained</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">layers</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="w">  </span><span class="c1"># Legacy field</span>
<span class="w">    </span><span class="nt">frozen</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w">        </span><span class="c1"># Legacy field</span>
</pre></div>
</div>
</section>
<section id="advanced-mlp-probe">
<h3>Advanced MLP Probe<a class="headerlink" href="#advanced-mlp-probe" title="Link to this heading">¶</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">experiments</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">run_name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;advanced_mlp&quot;</span>
<span class="w">    </span><span class="nt">run_config</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;configs/run_configs/example_run.yml&quot;</span>
<span class="w">    </span><span class="nt">pretrained</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;advanced_mlp&quot;</span>
<span class="w">      </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;mlp&quot;</span>
<span class="w">      </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;concat&quot;</span>
<span class="w">      </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pooled&quot;</span>
<span class="w">      </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_6&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_10&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="nt">freeze_backbone</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">      </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3e-4</span><span class="w">  </span><span class="c1"># Override global LR</span>
<span class="w">      </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class="w">        </span><span class="c1"># Override global batch size</span>
<span class="w">      </span><span class="nt">hidden_dims</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">1024</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">512</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">256</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="nt">dropout_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.15</span>
<span class="w">      </span><span class="nt">activation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;relu&quot;</span>
</pre></div>
</div>
</section>
<section id="sequence-lstm-probe">
<h3>Sequence LSTM Probe<a class="headerlink" href="#sequence-lstm-probe" title="Link to this heading">¶</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">experiments</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">run_name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence_lstm&quot;</span>
<span class="w">    </span><span class="nt">run_config</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;configs/run_configs/example_run.yml&quot;</span>
<span class="w">    </span><span class="nt">pretrained</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence_lstm&quot;</span>
<span class="w">      </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;lstm&quot;</span>
<span class="w">      </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;none&quot;</span>
<span class="w">      </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence&quot;</span>
<span class="w">      </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_8&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="nt">lstm_hidden_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="w">      </span><span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">      </span><span class="nt">bidirectional</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">      </span><span class="nt">max_sequence_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
<span class="w">      </span><span class="nt">use_positional_encoding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
</section>
</section>
<section id="migration-from-legacy-system">
<h2>Migration from Legacy System<a class="headerlink" href="#migration-from-legacy-system" title="Link to this heading">¶</a></h2>
<p>The new system automatically handles legacy configurations:</p>
<ol class="arabic simple">
<li><p><strong>Legacy fields still work</strong>: <code class="docutils literal notranslate"><span class="pre">layers</span></code> and <code class="docutils literal notranslate"><span class="pre">frozen</span></code> fields are automatically converted to <code class="docutils literal notranslate"><span class="pre">probe_config</span></code></p></li>
<li><p><strong>No breaking changes</strong>: Existing configurations continue to work without modification</p></li>
<li><p><strong>Gradual migration</strong>: You can update configurations one at a time</p></li>
</ol>
<section id="before-legacy">
<h3>Before (Legacy)<a class="headerlink" href="#before-legacy" title="Link to this heading">¶</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">experiments</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">run_name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;old_style&quot;</span>
<span class="w">    </span><span class="nt">layers</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;layer_12&quot;</span>
<span class="w">    </span><span class="nt">frozen</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</section>
<section id="after-new-style">
<h3>After (New Style)<a class="headerlink" href="#after-new-style" title="Link to this heading">¶</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">experiments</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">run_name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;new_style&quot;</span>
<span class="w">    </span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">      </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;linear&quot;</span>
<span class="w">      </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;mean&quot;</span>
<span class="w">      </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pooled&quot;</span>
<span class="w">      </span><span class="nt">target_layers</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;layer_12&quot;</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="nt">freeze_backbone</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</section>
</section>
<section id="training-parameter-overrides">
<h2>Training Parameter Overrides<a class="headerlink" href="#training-parameter-overrides" title="Link to this heading">¶</a></h2>
<p>Each probe can override global training parameters:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="c1"># ... other config ...</span>
<span class="w">  </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5e-4</span><span class="w">    </span><span class="c1"># Override global lr</span>
<span class="w">  </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class="w">          </span><span class="c1"># Override global batch_size</span>
<span class="w">  </span><span class="nt">train_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15</span><span class="w">       </span><span class="c1"># Override global train_epochs</span>
<span class="w">  </span><span class="nt">optimizer</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;adam&quot;</span><span class="w">      </span><span class="c1"># Override global optimizer</span>
<span class="w">  </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.001</span><span class="w">    </span><span class="c1"># Override global weight_decay</span>
</pre></div>
</div>
</section>
<section id="best-practices">
<h2>Best Practices<a class="headerlink" href="#best-practices" title="Link to this heading">¶</a></h2>
<section id="choose-appropriate-probe-types">
<h3>1. Choose Appropriate Probe Types<a class="headerlink" href="#choose-appropriate-probe-types" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Linear</strong>: Baseline performance, quick experiments</p></li>
<li><p><strong>MLP</strong>: Better performance, moderate complexity</p></li>
<li><p><strong>LSTM</strong>: Sequence modeling, moderate complexity</p></li>
<li><p><strong>Attention</strong>: Sequence modeling, higher complexity</p></li>
<li><p><strong>Transformer</strong>: Complex sequence modeling, highest complexity</p></li>
<li><p><strong>Weighted Probes</strong>: Enhanced versions that learn optimal weights for combining multiple layers</p>
<ul>
<li><p>Use when you want to leverage multiple layers efficiently</p></li>
<li><p>Better performance than concatenation with lower computational cost</p></li>
<li><p>Provides interpretability through learned layer weights</p></li>
</ul>
</li>
</ul>
</section>
<section id="layer-selection">
<h3>2. Layer Selection<a class="headerlink" href="#layer-selection" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Single layer</strong>: Use <code class="docutils literal notranslate"><span class="pre">[&quot;layer_12&quot;]</span></code> for final representations</p></li>
<li><p><strong>Multiple layers</strong>: Use <code class="docutils literal notranslate"><span class="pre">[&quot;layer_6&quot;,</span> <span class="pre">&quot;layer_8&quot;,</span> <span class="pre">&quot;layer_10&quot;,</span> <span class="pre">&quot;layer_12&quot;]</span></code> for hierarchical features</p></li>
<li><p><strong>Early layers</strong>: Use <code class="docutils literal notranslate"><span class="pre">[&quot;layer_1&quot;,</span> <span class="pre">&quot;layer_2&quot;,</span> <span class="pre">&quot;layer_3&quot;]</span></code> for low-level features</p></li>
</ul>
</section>
<section id="aggregation-strategy">
<h3>3. Aggregation Strategy<a class="headerlink" href="#aggregation-strategy" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Mean/Max</strong>: Good for classification tasks</p></li>
<li><p><strong>Concat</strong>: Better for complex tasks, requires larger probe networks</p></li>
<li><p><strong>None</strong>: Required for sequence-based probes and weighted probes</p></li>
<li><p><strong>Weighted Sum</strong>: Automatic with weighted probes when using <code class="docutils literal notranslate"><span class="pre">aggregation:</span> <span class="pre">&quot;none&quot;</span></code></p></li>
</ul>
</section>
<section id="input-processing">
<h3>4. Input Processing<a class="headerlink" href="#input-processing" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Pooled</strong>: Good for classification tasks</p></li>
<li><p><strong>Sequence</strong>: Required for sequence-based probes</p></li>
<li><p><strong>Flatten</strong>: Good for spatial features</p></li>
</ul>
</section>
</section>
<section id="validation">
<h2>Validation<a class="headerlink" href="#validation" title="Link to this heading">¶</a></h2>
<p>The system automatically validates configurations:</p>
<ul class="simple">
<li><p>Required parameters for each probe type</p></li>
<li><p>Compatibility between aggregation and input processing methods</p></li>
<li><p>Valid parameter ranges (positive integers, valid activation functions, etc.)</p></li>
<li><p>Layer name consistency</p></li>
</ul>
</section>
<section id="error-handling">
<h2>Error Handling<a class="headerlink" href="#error-handling" title="Link to this heading">¶</a></h2>
<p>Common validation errors and solutions:</p>
<section id="missing-required-parameters">
<h3>Missing Required Parameters<a class="headerlink" href="#missing-required-parameters" title="Link to this heading">¶</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Error: MLP probe requires hidden_dims</span>
<span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;mlp&quot;</span>
<span class="w">  </span><span class="c1"># Missing: hidden_dims</span>

<span class="c1"># Solution: Add required parameters</span>
<span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">probe_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;mlp&quot;</span>
<span class="w">  </span><span class="nt">hidden_dims</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">512</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">256</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
</section>
<section id="incompatible-configuration">
<h3>Incompatible Configuration<a class="headerlink" href="#incompatible-configuration" title="Link to this heading">¶</a></h3>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Error: cls_token aggregation requires sequence input_processing</span>
<span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;cls_token&quot;</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pooled&quot;</span>

<span class="c1"># Solution: Use sequence input_processing</span>
<span class="nt">probe_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">aggregation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;cls_token&quot;</span>
<span class="w">  </span><span class="nt">input_processing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sequence&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="performance-considerations">
<h2>Performance Considerations<a class="headerlink" href="#performance-considerations" title="Link to this heading">¶</a></h2>
<section id="memory-usage">
<h3>Memory Usage<a class="headerlink" href="#memory-usage" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Linear/MLP</strong>: Low memory usage</p></li>
<li><p><strong>LSTM</strong>: Moderate memory usage</p></li>
<li><p><strong>Attention/Transformer</strong>: Higher memory usage</p></li>
</ul>
</section>
<section id="training-speed">
<h3>Training Speed<a class="headerlink" href="#training-speed" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Linear</strong>: Fastest training</p></li>
<li><p><strong>MLP</strong>: Fast training</p></li>
<li><p><strong>LSTM</strong>: Moderate training speed</p></li>
<li><p><strong>Attention/Transformer</strong>: Slower training</p></li>
</ul>
</section>
<section id="inference-speed">
<h3>Inference Speed<a class="headerlink" href="#inference-speed" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Linear</strong>: Fastest inference</p></li>
<li><p><strong>MLP</strong>: Fast inference</p></li>
<li><p><strong>LSTM</strong>: Moderate inference speed</p></li>
<li><p><strong>Attention/Transformer</strong>: Slower inference</p></li>
</ul>
</section>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading">¶</a></h2>
<section id="common-issues">
<h3>Common Issues<a class="headerlink" href="#common-issues" title="Link to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>Out of Memory</strong>: Reduce batch size or use simpler probe types</p></li>
<li><p><strong>Slow Training</strong>: Use simpler probe types or reduce hidden dimensions</p></li>
<li><p><strong>Poor Performance</strong>: Try different aggregation methods or layer combinations</p></li>
<li><p><strong>Validation Errors</strong>: Check parameter compatibility and required fields</p></li>
</ol>
</section>
<section id="debug-mode">
<h3>Debug Mode<a class="headerlink" href="#debug-mode" title="Link to this heading">¶</a></h3>
<p>Enable debug logging to see detailed configuration validation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="future-extensions">
<h2>Future Extensions<a class="headerlink" href="#future-extensions" title="Link to this heading">¶</a></h2>
<p>The system is designed to be extensible:</p>
<ul class="simple">
<li><p><strong>New probe types</strong>: Easy to add new probe architectures</p></li>
<li><p><strong>Custom aggregations</strong>: Support for custom aggregation functions</p></li>
<li><p><strong>Advanced processing</strong>: More sophisticated input processing methods</p></li>
<li><p><strong>Hyperparameter optimization</strong>: Integration with hyperparameter search tools</p></li>
</ul>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="api_probes.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Probe API Documentation</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="examples.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Examples</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2026, Earth Species Project
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Flexible Probe System</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#probe-types">Probe Types</a><ul>
<li><a class="reference internal" href="#linear-probe-linear">1. Linear Probe (<code class="docutils literal notranslate"><span class="pre">&quot;linear&quot;</span></code>)</a></li>
<li><a class="reference internal" href="#mlp-probe-mlp">2. MLP Probe (<code class="docutils literal notranslate"><span class="pre">&quot;mlp&quot;</span></code>)</a></li>
<li><a class="reference internal" href="#lstm-probe-lstm">3. LSTM Probe (<code class="docutils literal notranslate"><span class="pre">&quot;lstm&quot;</span></code>)</a></li>
<li><a class="reference internal" href="#attention-probe-attention">4. Attention Probe (<code class="docutils literal notranslate"><span class="pre">&quot;attention&quot;</span></code>)</a></li>
<li><a class="reference internal" href="#transformer-probe-transformer">5. Transformer Probe (<code class="docutils literal notranslate"><span class="pre">&quot;transformer&quot;</span></code>)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#weighted-probe-types">Weighted Probe Types</a><ul>
<li><a class="reference internal" href="#weighted-linear-probe-weighted-linear">6. Weighted Linear Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_linear&quot;</span></code>)</a></li>
<li><a class="reference internal" href="#weighted-mlp-probe-weighted-mlp">7. Weighted MLP Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_mlp&quot;</span></code>)</a></li>
<li><a class="reference internal" href="#weighted-lstm-probe-weighted-lstm">8. Weighted LSTM Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_lstm&quot;</span></code>)</a></li>
<li><a class="reference internal" href="#weighted-attention-probe-weighted-attention">9. Weighted Attention Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_attention&quot;</span></code>)</a></li>
<li><a class="reference internal" href="#weighted-minimal-attention-probe-weighted-attention-minimal">10. Weighted Minimal Attention Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_attention_minimal&quot;</span></code>)</a></li>
<li><a class="reference internal" href="#weighted-transformer-probe-weighted-transformer">11. Weighted Transformer Probe (<code class="docutils literal notranslate"><span class="pre">&quot;weighted_transformer&quot;</span></code>)</a></li>
<li><a class="reference internal" href="#key-features-of-weighted-probes">Key Features of Weighted Probes</a></li>
<li><a class="reference internal" href="#requirements-for-weighted-probes">Requirements for Weighted Probes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#aggregation-methods">Aggregation Methods</a><ul>
<li><a class="reference internal" href="#mean"><code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code></a></li>
<li><a class="reference internal" href="#max"><code class="docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code></a></li>
<li><a class="reference internal" href="#concat"><code class="docutils literal notranslate"><span class="pre">&quot;concat&quot;</span></code></a></li>
<li><a class="reference internal" href="#cls-token"><code class="docutils literal notranslate"><span class="pre">&quot;cls_token&quot;</span></code></a></li>
<li><a class="reference internal" href="#none"><code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#input-processing-methods">Input Processing Methods</a><ul>
<li><a class="reference internal" href="#pooled"><code class="docutils literal notranslate"><span class="pre">&quot;pooled&quot;</span></code></a></li>
<li><a class="reference internal" href="#sequence"><code class="docutils literal notranslate"><span class="pre">&quot;sequence&quot;</span></code></a></li>
<li><a class="reference internal" href="#flatten"><code class="docutils literal notranslate"><span class="pre">&quot;flatten&quot;</span></code></a></li>
<li><a class="reference internal" href="#id1"><code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#configuration-examples">Configuration Examples</a><ul>
<li><a class="reference internal" href="#basic-linear-probe-legacy-style">Basic Linear Probe (Legacy Style)</a></li>
<li><a class="reference internal" href="#advanced-mlp-probe">Advanced MLP Probe</a></li>
<li><a class="reference internal" href="#sequence-lstm-probe">Sequence LSTM Probe</a></li>
</ul>
</li>
<li><a class="reference internal" href="#migration-from-legacy-system">Migration from Legacy System</a><ul>
<li><a class="reference internal" href="#before-legacy">Before (Legacy)</a></li>
<li><a class="reference internal" href="#after-new-style">After (New Style)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#training-parameter-overrides">Training Parameter Overrides</a></li>
<li><a class="reference internal" href="#best-practices">Best Practices</a><ul>
<li><a class="reference internal" href="#choose-appropriate-probe-types">1. Choose Appropriate Probe Types</a></li>
<li><a class="reference internal" href="#layer-selection">2. Layer Selection</a></li>
<li><a class="reference internal" href="#aggregation-strategy">3. Aggregation Strategy</a></li>
<li><a class="reference internal" href="#input-processing">4. Input Processing</a></li>
</ul>
</li>
<li><a class="reference internal" href="#validation">Validation</a></li>
<li><a class="reference internal" href="#error-handling">Error Handling</a><ul>
<li><a class="reference internal" href="#missing-required-parameters">Missing Required Parameters</a></li>
<li><a class="reference internal" href="#incompatible-configuration">Incompatible Configuration</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-considerations">Performance Considerations</a><ul>
<li><a class="reference internal" href="#memory-usage">Memory Usage</a></li>
<li><a class="reference internal" href="#training-speed">Training Speed</a></li>
<li><a class="reference internal" href="#inference-speed">Inference Speed</a></li>
</ul>
</li>
<li><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li><a class="reference internal" href="#common-issues">Common Issues</a></li>
<li><a class="reference internal" href="#debug-mode">Debug Mode</a></li>
</ul>
</li>
<li><a class="reference internal" href="#future-extensions">Future Extensions</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="_static/documentation_options.js?v=8d563738"></script>
    <script src="_static/doctools.js?v=fd6eb6e6"></script>
    <script src="_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    </body>
</html>